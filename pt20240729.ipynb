{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # 使用第一个GPU\n",
    "os.environ['TRANSFORMERS_OFFLINE']='1' # 离线的方式加载模型\n",
    "os.environ['DWAN _DISABLED'] = 'true' #模型规模较小，，单GPU，不需要zeRO优化来减少内存占用\n",
    "warnings.filterwarnings(\"ignore\")   #忽略所有警告\n",
    "#ipynb 最方便\n",
    "###  -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# pip install modelscope   bitsandbytes\n",
    "# pip install transformers datasets evaluate peft accelerate gradio optimum sentencepiece  \n",
    "import torch\n",
    "from modelscope import (AutoModelForCausalLM, BitsAndBytesConfig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315119488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# python -m \n",
    "\n",
    "#BitsAndBytesconfig;配置模型最化参数的头\n",
    "#4位量化(从32位浮点数转换为4位整数)，对比量化前后参数量的变化\n",
    "_bnb_config = BitsAndBytesConfig(load_in_4bit=True, # 权重被加载为4位整数\n",
    "                                bnb_4bit_use_double_quant=True,#双量化方案:对权重和激活值进行量化\n",
    "                                bnb_4bit_quant_type=\"nf4\",\n",
    "                                # 量化(允许权重和激活值以不同的精度进行量化,\n",
    "                                bnb_4bit_compute_dtype=torch.float32)# 量化后的int4的计算仍然在32位浮点精度(torch.float32)上进行,\n",
    "#从预训练模型库中加载\"0wen/owen2-0.5B\"模型\n",
    "_model = AutoModelForCausalLM.from_pretrained(\"Qwen2-0.5B-Instruct\",\n",
    "                                                # 少cpu内存的使用\n",
    "                                                low_cpu_mem_usage=True,#将量化配置_bnb config应用于模型加载\n",
    "                                                quantization_config= _bnb_config)\n",
    "#计算模型的总参数量# \n",
    "print(f\"{sum(p.numel()for p in _model.parameters())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105043--ä½łæĺ¯\n",
      "100165--è°ģ\n",
      "30--?\n",
      "99242--çĪ±\n",
      "100165--è°ģ\n",
      "100165--è°ģ\n",
      "0--!\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoTokenizer, AutoModel\n",
    "#自动识别并加载与\"Qwen/Qwen2-0.5B\"模型匹配的的分词器\n",
    "# #将文本转换为模型可以理解的数字序列(即ids)\n",
    "# 词汇表:ids(索引)--tokens(词元\n",
    "_tokenizer = AutoTokenizer.from_pretrained(\"Qwen2-0.5B-Instruct\")\n",
    "##分词器将文本分割成词元，将词元转换为模型可以理解的IDS\n",
    "ids = _tokenizer.encode(\"你是谁?爱谁谁!\",return_tensors=\"pt\")\n",
    "tokens =_tokenizer.convert_ids_to_tokens(ids[0])\n",
    "for id, token in zip(ids[0],tokens):\n",
    "    print(f\"{id}--{token}\")\n",
    "    #词向量#\n",
    "#_model = AutoModel.from_pretrained(\"Qwen1.5-0.5B-Chat\")\n",
    "# 将ids转换为词向量# \n",
    "# embeddings =_model(ids)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "#加载ison格式的训练数据集\n",
    "_dataset = load_dataset(\"json\", data_files=\"./data.json\", split=\"train\")\n",
    "#预处理数据集函数\n",
    "def preprocess_dataset(example):\n",
    "    MAX_LENGTH = 256 # 最大长度\n",
    "    _input_ids,_attention_mask,_labels =[],[],[]# 初始化输入 ids、注意力掩码、标签列表\n",
    "    #使用分词器对指令和响应进行编码\n",
    "    _instruction = _tokenizer(f\"user: {example['instruction']}Assistant:\", add_special_tokens=False)\n",
    "    print(_instruction)\n",
    "    _response =_tokenizer(example[\"output\"]+ _tokenizer.eos_token, add_special_tokens=False)\n",
    "    #拼接指令和响应的输入ids\n",
    "    _input_ids = _instruction[\"input_ids\"]+ _response[\"input_ids\"]\n",
    "    #拼接指令和响应的注意力掩码\n",
    "    _attention_mask = _instruction[\"attention_mask\"]+ _response[\"attention_mask\"]\n",
    "    #拼接标签，这里将第一个指令的标签设置为-100\n",
    "    _labels =[-100]* len(_instruction[\"input_ids\"])+ _response[\"input_ids\"]\n",
    "    #如果拼接后的输入ids长度超过最大长度，则进行截\n",
    "    if len(_input_ids)> MAX_LENGTH:\n",
    "        _input_ids = _input_ids[:MAX_LENGTH]\n",
    "        _attention_mask = _attention_mask[:MAX_LENGTH]\n",
    "        _labels =_labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": _input_ids,\n",
    "        \"attention_mask\":_attention_mask,\n",
    "        \"labels\":_labels\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#移除原始数据集中的列，只保留预处理后的数据。\n",
    "_dataset = _dataset.map(preprocess_dataset, remove_columns=_dataset.column_names)\n",
    "_dataset = _dataset.shuffle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (LoraConfig,get_peft_model,TaskType)\n",
    "#定义微调配置\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                    # 因果语言模型(模型的输出只依赖于当前的输入，而不依赖于未来的输入)\n",
    "                    r=8,#LORA缩放因子，控制模型的稀疏性\n",
    "                    target_modules=\"all-linear\") # 所有权重都参与训练\n",
    "#自动识别并加载与\"owen/owen2-0.5B\"模型匹配的微调配置\n",
    "_model = get_peft_model(_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12/60 [00:05<00:21,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0686, 'grad_norm': 3.4916467666625977, 'learning_rate': 4e-05, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 24/60 [00:11<00:19,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6475, 'grad_norm': 1.9158254861831665, 'learning_rate': 3e-05, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 36/60 [00:17<00:10,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1099, 'grad_norm': 0.43174120783805847, 'learning_rate': 2e-05, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 48/60 [00:23<00:05,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0228, 'grad_norm': 0.17582397162914276, 'learning_rate': 1e-05, 'epoch': 48.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:28<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.011, 'grad_norm': 0.13066762685775757, 'learning_rate': 0.0, 'epoch': 60.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:28<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 28.8258, 'train_samples_per_second': 4.163, 'train_steps_per_second': 2.081, 'train_loss': 0.5719777956604958, 'epoch': 60.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=0.5719777956604958, metrics={'train_runtime': 28.8258, 'train_samples_per_second': 4.163, 'train_steps_per_second': 2.081, 'total_flos': 8347327856640.0, 'train_loss': 0.5719777956604958, 'epoch': 60.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments,Trainer,DataCollatorForSeq2Seq\n",
    "\n",
    "#定义训练参数\n",
    "_training_args = TrainingArguments(output_dir=\"checkpoints/qlora\",\n",
    "                                    # 训练结果的存储目录\n",
    "                                    run_name=\"qlora_study\",#运行的名称\n",
    "                                    per_device_train_batch_size=10, # batch_size批处理大小\n",
    "                                    num_train_epochs=100, #训练的轮次\n",
    "                                    save_steps=12, # 保存检查点的轮次步数\n",
    "                                    logging_steps=12, #写日志的轮次步数\n",
    "                                    report_to=\"none\",#指定不报告任何日志\n",
    "                                    optim=\"paged_adamw_32bit\") # 指定优化器\n",
    "# 创建 Trainer 对象\n",
    "trainer = Trainer(model=_model,\n",
    "                  #指定模型对象\n",
    "                  args=_training_args, #指定训练参数\n",
    "                  train_dataset=_dataset,#指定训练数据集\n",
    "                  data_collator=DataCollatorForSeq2Seq(tokenizer=_tokenizer,padding=True),\n",
    "                    ) # 指定数据集的收集器\n",
    "\n",
    "#调用 Trainer 的 train 方法开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'User: 你是谁? Assistant: 你好！我是一个高中生。'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./qwenlora_pt\\\\tokenizer_config.json',\n",
       " './qwenlora_pt\\\\special_tokens_map.json',\n",
       " './qwenlora_pt\\\\vocab.json',\n",
       " './qwenlora_pt\\\\merges.txt',\n",
       " './qwenlora_pt\\\\added_tokens.json',\n",
       " './qwenlora_pt\\\\tokenizer.json')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将微调后的模型合并回原始模型，并最终保存更新后的模型\n",
    "from transformers import pipeline ,AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "#加载模型#使用\n",
    "#AutoModelForcausalLm.from_pretrained方法自动识别并加载与\"Qwen/Qwen2-8.5B\"模型匹配的模型\n",
    "_model = AutoModelForCausalLM.from_pretrained(\"Qwen2-0.5B-Instruct\",\n",
    "                                              quantization_config=_bnb_config,# 指定量化配置\n",
    "                                              low_cpu_mem_usage=True)#指定减少CPU内存使用\n",
    "#使用PeftModel.from pretrained方法自动识别并加载与 model匹配的微调配置\n",
    "# # model参数指定原始模型对象，model id参数指定微调配置的ID\n",
    "peft_model = PeftModel.from_pretrained(model= _model, model_id=\"checkpoints/qlora/checkpoint-60\")\n",
    "#使用pipeline方法创建一个管道对象，用于生成文本#pipeline方法需要三个参数:任务类型、模型对象、分词器对象\n",
    "pipe = pipeline(\"text-generation\", model=peft_model, tokenizer= _tokenizer)\n",
    "\n",
    "#使用pipeline管道生成文本\n",
    "response = pipe(\"User: 你是谁? Assistant: \")#[0]['generated_text']\n",
    "# response = pipe(\"instruction: 你是谁? output: \")#[0]['generated_text'] 不要这个\n",
    "# pipe(f\"Ut 你是谁 ？\")\n",
    "print(response)\n",
    "############save\n",
    "output_dir =\"./qwenlora_pt\"\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(output_dir)\n",
    "_tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
